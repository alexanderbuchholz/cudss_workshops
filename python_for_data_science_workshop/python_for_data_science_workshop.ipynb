{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Science\n",
    "For starting the workshop, please make sure you have anaconda installed. We will use python 3 for this workshop. \n",
    "\n",
    "The material for the course is available at \n",
    "https://github.com/alexanderbuchholz/cudss_workshops/blob/master/python_for_data_science_workshop/python_for_data_science_workshop.ipynb\n",
    "\n",
    "\n",
    "## Main tools in python for data science\n",
    "The main tools/libraries that you need to learn for doing data science are the following (this is of course a bit subjective!):\n",
    "1. Numpy, (numeric python). This library handles matrices, vectors and matrix-vector calculations. The underlying code is written in C. \n",
    "2. Pandas. It is a data handling library that allows you to load your data, visualize and preprocess it. Essential for gettig the first insights! \n",
    "3. Matplotlib and seaborn. Two libraries that can be used to make nice plots of your data. \n",
    "4. Sklearn (scikit learn). To run all your fancy models. All models are set up in the same way: you create your model, fit (train) it and make predictions of unseen data. \n",
    "\n",
    "## Ipython (jupyter) notebooks\n",
    "Notebooks are a good way of playing around with your data and test different things. However, be careful as you can execute your cells in various orders. \n",
    "You can start with a notebook and easily turn it into something presentable using Markdown choosing different cell types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# this line lets us have the plot shown without calling \"plt.show()\"\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The import \"xxx\" as \"x\" is the standard in python. Stick to the naming conventions, as this will make your code more readable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now read a dataset, more precisely the Pima diabetes dataset. This data was is a benchmark for testing machine learning algorithms (nowadays, it is considered too simple). It contains information on a group of Pima, a native american tribe. In the dataset, there are 768 female individuals, some of which suffer from diabetes. Our aim is to predict whether an individual suffers from diabetes given other indicators (bmi, pregnancy record...). \n",
    "For more information see here: \n",
    "https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "The variables in the data set are: \n",
    "\n",
    "Pregnancies - Number of times pregnant\n",
    "\n",
    "Glucose - Plasma glucose concentration after 2 hours (oral glucose tolerance test)\n",
    "\n",
    "BloodPressure - Diastolic blood pressure (mm Hg)\n",
    "\n",
    "SkinThickness - Triceps skin fold thickness (mm)\n",
    "\n",
    "Insulin - 2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "BMI - Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "DiabetesPedigreeFunction - Diabetes pedigree function\n",
    "\n",
    "Age - Age (years)\n",
    "\n",
    "Outcome - Class variable (0 or 1) 268 of 768 are 1, the others are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all = pd.read_csv(\"pima-indians-diabetes.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this command do? Try to find out more by using \"help(pd.read_csv)\".\n",
    "\n",
    "If the previous command does not work, try to specify the path where the file is located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.columns = ['num_pregnant', 'glucose', 'pressure', 'skin', 'insulin', 'bmi', 'pedigree' , 'age', 'diab_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What problems do you see here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.skin.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.insulin.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "Look at the other variables and see if you can find anything suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a more systematic approach: replace zero values by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.iloc[:,[1,2,3,4,5]] = pima_all.iloc[:,[1,2,3,4,5]].replace(0, np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this command do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to understand what drives diabetes.\n",
    "We will use a different library that yields some nice plots, called seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"diab_class\", y=\"glucose\", data=pima_all)\n",
    "plt.xlabel('Diabetes status') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.diab_class.mean() # what does this number tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Do the same thing for the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.groupby('diab_class').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all[['glucose', 'pressure', 'diab_class']].boxplot(by='diab_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the factors that drive diabetes based on the first two plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='glucose', y='insulin', data=pima_all, hue='diab_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='bmi', y='insulin', data=pima_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pima_all[pima_all['diab_class']==0].pedigree)\n",
    "sns.distplot(pima_all[pima_all['diab_class']==1].pedigree)\n",
    "#plt.savefig('two_histograms.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we handle missing values? \n",
    "One way is to drop all lines with missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better approach: impute missing values using for example the mean or the most frequent value. We will use the imputing method provided by pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_all.fillna(pima_all.mean(), inplace=True) # can you explain what this command does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pima_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "First step: split the data into a train and a test dataset.\n",
    "sklearn is a library that contains a lot of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to transform the data first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pima_all.iloc[:,0:8].values\n",
    "y = pima_all.iloc[:,8].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "What is wrong with this approach? Hint: look at what we did before, how did we preprocess the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the first model using only the first three variables: num_pregnant, glucose, pressure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression:\n",
    "### What is a logistic regression?\n",
    "A logistic regression is a model that assigns to every outcome (diabetes or not) a probability between 0 and 1. \n",
    "The idea is that every individual that we observe can be represented as the observation of a coin flip (either 0 or 1). However every coin is different for all individiuals. That means every individual has its own unique coin. The properties of this unique coin are determined by the observed covariates (the bmi for instance). We assume that there is a shared way of how the individual covariates influence the properties of the coin. This shared structure allows to learn the parameters that govern the model:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y_i = 1| X_i) = logit(\\sum_{j=1}^p x_{i,j} \\beta_j)\n",
    "$$\n",
    "and $\\beta_j$ is the same accross individuals. \n",
    "\n",
    "Thus, a logistic regression allows us to model the individual probability of having diabetes. \n",
    "\n",
    "For more details see here \n",
    "https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticmodel = LogisticRegression() # we initiate a model by calling the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticmodel.fit(X_train[:,0:3], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about how the model is trained, look at maximum likelihood estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = logisticmodel.predict(X_train[:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "Try to understand what these different evaluation metrics do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = logisticmodel.predict(X_test[:,0:3])\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_test_pred), f1_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_test_pred), accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "Also use the other variables. What accuracy do you obtain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use another regression model, the random forest classification (look up how to use it). What is the best score that you get? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Can you think of a way of how to fix the imputation problem? \n",
    "Look at imputation in sklearn and pipelines. This allows you to find a better way of imputation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to go further: \n",
    "\n",
    "## Learning using moocs: \n",
    "https://www.coursera.org/learn/python-data-analysis\n",
    "or using other ressources on coursera\n",
    "\n",
    "## Learning using kaggle\n",
    "Kaggle has lot of material, that can get you started. \n",
    "For example you might want to look at \n",
    "https://www.kaggle.com/learn/overview\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
